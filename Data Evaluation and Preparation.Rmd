###Liver Diseases have been a widespread issue in many countries. Third World countries suffer from a sparsity of doctors, leading to difficulty in diagnosing patients. With the advent of machine learning technology, we can now try to automate the process of diagnosing patients based on their test results. This saves the doctors and patients a lot of time, and those that are in need of attention can be tended to. This is a simple attempt to solve this calssification problem using different algorithms.


Calling the required libraries:

```{r, results='hide', warning=FALSE, comment=NA, message=FALSE}
library(ggplot2) 
library(readr)
library(psych)
library(corrplot)
library(ggpubr)
library(caret)
```

Reading the dataset file:
```{r, results='hide', warning=FALSE, comment=NA, message=FALSE}
liver_data <- read_csv("indian_liver_patient.csv")
```

Now we explore the dataset to find anomalies and get a better understanding of what course of action to follow:

```{r}
str(liver_data) 
summary(liver_data) 
```

From these operations we see that there are 10 attributes (dataset being our target variable), and 583 observations. We also see that there are four null values, which need to be dealt with before model building. Hence eliminating the null values:

```{r}
liver_data <- liver_data[complete.cases(liver_data), ]
#Change 2 to 0 in target variable 
liver_data[liver_data$Dataset==2,]$Dataset<-0
```

Verifying whether all the anamolies are dealt with:
```{r}
str(liver_data)
summary(liver_data)
```

The data is now ready. Exploring further to better understand the dataset.
```{r}
#Age
par(mfrow=c(1,2))
hist(liver_data$Age, main = 'Age distribution')
boxplot(liver_data$Age, main="Boxplot of Age")

#Gender
par(mfrow=c(1,1))
barplot(table(liver_data$Gender))

#Dataset(target variable)
par(mfrow=c(1,2))
barplot(table(liver_data$Dataset))
par(mfrow=c(1,1))

#Evaluating the number of healthy and unhealthy patients in the dataset(1=unhealthey, 0=healthy)
table(liver_data$Dataset)
```

From these plots we can see that there are more males than females in the data set, and the most of the people are between 40-50 years old.

Evaluating the correlations between the features

```{r}
# Non-numeric fields are not considered
cor_matrix <- cor(liver_data[,-c(2,11)]) 
cor_matrix
corrplot(cor_matrix,title = "Correlation Matrix",tl.col = "black",type = "upper")
```

Positive correlations are displayed in blue and negative correlations in red color. Color intensity and the size of the circle are proportional to the correlation coefficients.On the right side of the correlogram, the legend color shows the correlation coefficients and the corresponding colors. From the plot it is eveident that there is a high correlation between the following pair : Total_Bilrubin, Direct Bilrubin; Alamine_Amniotransferase, Aspartate_Amniotranferase; Total_Proteins, Albumin. Exploring these correlations further.
```{r}
par(mfrow=c(2,2))
ggscatter(data = liver_data, x = "Total_Bilirubin", y = "Direct_Bilirubin", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Total Bilirudbin", ylab = "Direct Bilirubin")

ggscatter(data = liver_data, x = "Total_Protiens", y = "Albumin", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Total Protiens", ylab = "Albumin")

ggscatter(data = liver_data, x = "Alamine_Aminotransferase", y = "Aspartate_Aminotransferase", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Alamine_Aminotransferase", ylab = "Aspartate_Aminotransferase")

ggscatter(data = liver_data, x = "Albumin", y = "Albumin_and_Globulin_Ratio", 
          add = "reg.line", conf.int = TRUE, 
          cor.coef = TRUE, cor.method = "pearson",
          
          xlab = "Albumin", ylab = "Albumin_and_Globulin_Ratio")
```
These pairs are highly correlated and thus only 3 out of the 6 features need to be selected as they would have the same impact on the model. Eliminating Gender, Direct_Bilrubin, Alkaline_Phosphotase, Asparate_Amino_Transferase, in an attempt to build more robust models.
```{r}
#Select the columns we need as predictors
str(liver_data)
liver_data <- liver_data[,c(1,3,6,8,9,10,11)]
colnames(liver_data)
```

Now dividing the dataset into training and testing sets, for SVM and KNN
```{r}
# 70% of the sample size
smp_size <- floor(0.70 * nrow(liver_data))

## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(liver_data)), size = smp_size)
train_data <- liver_data[train_ind, ]
test_data <- liver_data[-train_ind, ]

str(train_data)
str(test_data)
str(liver_data)
```
Now that the data has been prepared, the model construction process can be initiated. Since this is a classificaiton problem, the implementation of KNN, CART, and Logistic Regression.

Building the KNN model:

```{r}

############################################################################################
### K - Nearest Neighbours
############################################################################################

train_knn <- train_data
test_knn <- test_data
str(train_knn)
str(test_knn)

#Our target variable is 'Dataset' variable
#To remove target variable from training and testing data
train_knn2 <- train_knn[, -7]
train_knn2 <- scale(train_knn2)
test_knn2 <- test_knn[ ,-7]
test_knn2 <- scale(test_knn2)
#To check if traget variable has been removed
str(train_knn2)
str(test_knn2)

train_knn_label <- train_knn$Dataset
test_knn_label <- test_knn$Dataset
str(train_knn_label)
str(test_knn_label)
```
#Building a KNN model
```{r ,warning=FALSE, message=FALSE, comment=NA, result='hide'}
library(class)
library(pROC)
```

```{r}
pred_knn <- knn(train = train_knn2, test = test_knn2, cl = train_knn_label, k = 1)
table_knn <- table(test_knn_label, pred_knn)
total <- table_knn[1,1] + table_knn[2,2]
accuracy <- (total/174)*100
accuracy


#Choosing best value of K 
k1 <- knn(train = train_knn2, test = test_knn2, cl = train_knn_label, k=1)
table1 <- table(k1, test_knn_label)
TotalCorrect1 <- table1[1,1] + table1[2,2]
Accuracy1 <- (TotalCorrect1/174)*100
print(Accuracy1)


k3 <- knn(train = train_knn2, test = test_knn2, cl = train_knn_label, k=3)
table3 <- table(k3, test_knn_label)
TotalCorrect3 <- table3[1,1] + table3[2,2]
Accuracy3 <- (TotalCorrect3/174)*100
print(Accuracy3)

k5 <- knn(train = train_knn2, test = test_knn2, cl = train_knn_label, k=5)
table5 <- table(k5, test_knn_label)
TotalCorrect5 <- table5[1,1] + table5[2,2]
Accuracy5 <- (TotalCorrect5/174)*100
print(Accuracy5)

k7 <- knn(train = train_knn2, test = test_knn2, cl = train_knn_label, k=7)
table7 <- table(k7, test_knn_label)
TotalCorrect7 <- table7[1,1] + table7[2,2]
Accuracy7 <- (TotalCorrect7/174)*100
print(Accuracy7)

k9 <- knn(train = train_knn2, test = test_knn2, cl = train_knn_label, k=9)
table9 <- table(k9, test_knn_label)
TotalCorrect9 <- table9[1,1] + table9[2,2]
Accuracy9 <- (TotalCorrect9/174)*100
print(Accuracy9)

k11 <- knn(train = train_knn2, test = test_knn2, cl = train_knn_label, k=11)
table11 <- table(k11, test_knn_label)
TotalCorrect11 <- table11[1,1] + table11[2,2]
Accuracy11 <- (TotalCorrect11/174)*100
print(Accuracy11)

k13 <- knn(train = train_knn2, test = test_knn2, cl = train_knn_label, k=13)
table13 <- table(k13, test_knn_label)
TotalCorrect13 <- table13[1,1] + table13[2,2]
Accuracy13 <- (TotalCorrect13/174)*100
print(Accuracy13)

k15 <- knn(train = train_knn2, test = test_knn2, cl = train_knn_label, k=15)
table15 <- table(k15, test_knn_label)
TotalCorrect15 <- table15[1,1] + table15[2,2]
Accuracy15 <- (TotalCorrect15/174)*100
print(Accuracy15)


k17 <- knn(train = train_knn2, test = test_knn2, cl = train_knn_label, k=17)
table17 <- table(k17, test_knn_label)
TotalCorrect17 <- table17[1,1] + table17[2,2]
Accuracy17 <- (TotalCorrect17/174)*100
print(Accuracy17)

k19 <- knn(train = train_knn2, test = test_knn2, cl = train_knn_label, k=19)
table19 <- table(k19, test_knn_label)
TotalCorrect19 <- table19[1,1] + table19[2,2]
Accuracy19 <- (TotalCorrect19/174)*100
print(Accuracy19)

k21 <- knn(train = train_knn2, test = test_knn2, cl = train_knn_label, k=21)
table21 <- table(k21, test_knn_label)
TotalCorrect21 <- table21[1,1] + table21[2,2]
Accuracy21 <- (TotalCorrect21/174)*100
print(Accuracy21)

k23 <- knn(train = train_knn2, test = test_knn2, cl = train_knn_label, k=23)
table23 <- table(k23, test_knn_label)
TotalCorrect23 <- table23[1,1] + table23[2,2]
Accuracy23 <- (TotalCorrect23/174)*100
print(Accuracy23)

k25 <- knn(train = train_knn2, test = test_knn2, cl = train_knn_label, k=25)
table25 <- table(k25, test_knn_label)
table25
TotalCorrect25 <- table25[1,1] + table25[2,2]
Accuracy25 <- (TotalCorrect25/174)*100
print(Accuracy25)

#Ploting K values and Accuracy
plot_accuracy <- c(Accuracy1, Accuracy3, Accuracy5, Accuracy7, Accuracy9, 
                   Accuracy11, Accuracy13, Accuracy15, Accuracy17, Accuracy19,
                   Accuracy21,Accuracy23,Accuracy25)
plot_Klabels <- c("K=1","K=3","K=5","K=7","K=9",
                  "K=11","K=13","K=15","K=17","K=19",
                  "K=21","K=23","K=25")
K_values <- c(1,3,5,7,9,11,13,15,17,19,21,23,25)
plot(x=K_values,y=plot_accuracy,xlab ="K value", ylab = "Accuracy in %", 
     main="Accuracy of KNN model with varying K values")
text(x=K_values,y=plot_accuracy,labels=plot_Klabels, pos =1 )


#Metrics for best K value

error_rate <- (table25[1,2] +  table25[2,1]) / 174 


false_pos_rate <- table25[2,1]/(table25[2,1] + table25[2,2])

#ROC Curve 
k25 <- knn(train = train_knn2, test = test_knn2, cl = train_knn_label, k=25, prob = TRUE)
k25_prob<-attr(k25, "prob")
roc(test_knn_label,k25_prob, plot=TRUE,legacy.axes = TRUE, percent = TRUE, main = "ROC Curve for KNN" ,xlab="False Positive Percentage", ylab ="True Positive Percentage", print.auc = TRUE)
```
The error rate is `r error_rate`.

The false positive rate is `r false_pos_rate`.

Running a Logistic Regression Model:
```{r, warning=FALSE, message=FALSE, comment=NA, result='hide'}
#Libraries:
library(car)
library(caret)
library(lattice)
library(party)
```

```{r, warning=FALSE}
#Logistic regression Model:
#Creating a "0-1" column:
patients<-liver_data
colnames(patients)
#Creating the regression model:
logmod<- glm(Dataset~.,data = patients,family = binomial())
#Predicting the values:
pred<-predict(logmod, type = "response")
#Creating a confusion matrix to evaluate the results:
confusionMatrix(as.factor(patients$Dataset),as.factor(ifelse(pred>0.5,1,0)))
#Plotting the ROC Curve:
roc(patients$Dataset,pred, plot=TRUE, percent = TRUE, legacy.axes=TRUE, main="ROC Curve for Logistic Regression" ,xlab = "False Positive Perecentage", ylab="True Positive Rate", print.auc=TRUE)

```

Implementing a CART model on the dataset:
```{r, warning=FALSE, message=FALSE, comment=NA, result='hide'}
#Decision Tress:
#library
library(rpart)
library(rpart.plot)
library(maptree)
library(cluster)
```

```{r}
#Creating the model
dtmodel<-rpart(Dataset~.,data = patients, method = "class")
printcp(dtmodel)
rpart.plot(dtmodel)

#Decision Tree Plot
plot(dtmodel, uniform=TRUE, main="Classification Tree for Patient Records")

#Testing the model:
dtpred<-predict(dtmodel,type = "class")

#Confusion Matrix:
confusionMatrix(dtpred,as.factor(patients$Dataset))

#ROC Plot
roc(patients$Dataset,as.numeric(dtpred), plot=TRUE, percent = TRUE, legacy.axes=TRUE, main="ROC Curve for CART ",xlab = "False Positive Perecentage", ylab="True Positive Rate", print.auc=TRUE)

```

From the three fitted models, it is evident that the best model is the logistic regression model, since it has the highest AUC of 75.8%. Based on this an appropriate cut-off can be selected using data extracted from the ROC curve, making a trade off between the Falase Positive Rate and the True Positive Rate. In this case study, the goal should be to reduce the number of false negative cases, even though this may decrease the accuracy of the model. This is because, missing out on a patient's disease, could prove fatal for the patient, and is much worse than incorrectly predicting the presence of a liver disease. 

### Conclusion and Scope:
This is a promosing project, that could be worked on further to create better models and eventually be good enough to implement in the the real world, saving doctors' precious time around the globe, and help save lives. As we were pressed on time while working on this project, we did not fine tune our models. If given more time, we could have put in more effort on feature selection, and model tuning, which would have resulted in better results. The data set consisted of mainly male patients and was too small to better train the models.For future work, more data needs to be collected to get a more rounded dataset and use better machine learning algorithms to tackle this problem.



